{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Language Modeling Task: assign a probbaility for the likelihood of a given word to follow a sequence of words."
      ],
      "metadata": {
        "id": "5jYQ1TYEJhDU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AQ0NDigG77J2"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from tempfile import TemporaryDirectory\n",
        "from typing import Tuple\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        # multi-head attention defines how much focus should be given to each token when aggregating information from the sequence\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        # pass the transformer encoder layers to transformer encoder\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers) # it has multiple layers of TransformerEncoderLayer (see above)\n",
        "        self.embedding = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        # to produce a probability distribution over output words\n",
        "        self.linear = nn.Linear(d_model, ntoken) # produce a unnormalized logits (later passed to cross entropy loss - so we do not use softmax)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.linear.bias.data.zero_()\n",
        "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model) # sequence of tokens are passed to embedding layer\n",
        "        src = self.pos_encoder(src) # accound for the order of the word\n",
        "        if src_mask is None: # the mask is necessary because any tokens on the future positions should be masked\n",
        "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(src.device)\n",
        "        # predictions for position i can depend only on the known outputs at positions less than i.\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.linear(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to inject info about the position of tokens in the sequence\n",
        "# the encodings have the same dim as the embeddings so they can be summed\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "U3IY2uRYFB5j"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load and Batch Data"
      ],
      "metadata": {
        "id": "aqKZxRkFQLAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "train_file = 'train.txt'\n",
        "test_file = 'test.txt'\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Build vocabulary from train data\n",
        "with open(train_file, 'r') as train_iter:\n",
        "    vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "    vocab.set_default_index(vocab['<unk>'])\n",
        "\n",
        "# Process train data and split into train and validation sets\n",
        "with open(train_file, 'r') as train_iter:\n",
        "    train_data_raw = train_iter.readlines()\n",
        "\n",
        "split_ratio = 0.8\n",
        "train_size = math.floor(len(train_data_raw) * split_ratio)\n",
        "train_data_raw, val_data_raw = train_data_raw[:train_size], train_data_raw[train_size:]\n",
        "\n",
        "train_data = data_process(train_data_raw)\n",
        "val_data = data_process(val_data_raw)\n",
        "\n",
        "# Process test data\n",
        "with open(test_file, 'r') as test_iter:\n",
        "    test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# it arranges the data into batch_size columns (trimmed if not evenly divisible)\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Arguments:\n",
        "        data: Tensor, shape ``[N]``\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape ``[N // bsz, bsz]`` \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ],
      "metadata": {
        "id": "fcQWOpC2FOmE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bptt = 35\n",
        "# it generates a pair of input-target sequences, subdiving data into chucks of lenght bptt\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i + seq_len]\n",
        "    target = source[i + 1:i + 1 + seq_len].reshape(-1)\n",
        "    return data, target"
      ],
      "metadata": {
        "id": "KMSBMzoxFUyv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
        "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
        "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcoKIn35FX-p",
        "outputId": "127923fa-30f1-440f-8ddc-ed921d6fb3ef"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        output = model(data)\n",
        "        output_flat = output.view(-1, ntokens)\n",
        "        loss = criterion(output_flat, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "metadata": {
        "id": "ZOectCz0FdVF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            output = model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "\n",
        "            _, predicted = torch.max(output_flat, 1)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "    return total_loss / (len(eval_data) - 1), correct / total\n",
        "\n",
        "\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs = 3\n",
        "\n",
        "with TemporaryDirectory() as tempdir:\n",
        "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(model)\n",
        "        val_loss,_ = evaluate(model, val_data)\n",
        "        val_ppl = math.exp(val_loss)\n",
        "        elapsed = time.time() - epoch_start_time\n",
        "        print('-' * 89)\n",
        "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "        scheduler.step()\n",
        "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states\n",
        "\n",
        "test_loss,accuracy = evaluate(model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)\n",
        "\n",
        "print(f'Accuracy: {accuracy*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzlF0oNHFjji",
        "outputId": "b328172f-8a9a-4c68-8df0-b312dd7e50d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2335 batches | lr 5.00 | ms/batch 846.72 | loss  8.03 | ppl  3057.10\n",
            "| epoch   1 |   400/ 2335 batches | lr 5.00 | ms/batch 847.97 | loss  6.80 | ppl   895.62\n",
            "| epoch   1 |   600/ 2335 batches | lr 5.00 | ms/batch 852.85 | loss  6.48 | ppl   654.83\n",
            "| epoch   1 |   800/ 2335 batches | lr 5.00 | ms/batch 855.21 | loss  6.33 | ppl   563.05\n",
            "| epoch   1 |  1000/ 2335 batches | lr 5.00 | ms/batch 845.76 | loss  6.16 | ppl   471.50\n",
            "| epoch   1 |  1200/ 2335 batches | lr 5.00 | ms/batch 845.50 | loss  6.15 | ppl   470.81\n",
            "| epoch   1 |  1400/ 2335 batches | lr 5.00 | ms/batch 849.73 | loss  6.09 | ppl   442.77\n",
            "| epoch   1 |  1600/ 2335 batches | lr 5.00 | ms/batch 836.72 | loss  6.06 | ppl   429.03\n",
            "| epoch   1 |  1800/ 2335 batches | lr 5.00 | ms/batch 843.18 | loss  6.00 | ppl   404.87\n",
            "| epoch   1 |  2000/ 2335 batches | lr 5.00 | ms/batch 871.05 | loss  5.99 | ppl   400.50\n",
            "| epoch   1 |  2200/ 2335 batches | lr 5.00 | ms/batch 867.92 | loss  5.94 | ppl   378.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2203.17s | valid loss  6.12 | valid ppl   456.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2335 batches | lr 4.75 | ms/batch 884.13 | loss  5.87 | ppl   352.80\n",
            "| epoch   2 |   400/ 2335 batches | lr 4.75 | ms/batch 893.26 | loss  5.82 | ppl   336.97\n",
            "| epoch   2 |   600/ 2335 batches | lr 4.75 | ms/batch 888.23 | loss  5.73 | ppl   309.32\n",
            "| epoch   2 |   800/ 2335 batches | lr 4.75 | ms/batch 907.60 | loss  5.76 | ppl   315.81\n",
            "| epoch   2 |  1000/ 2335 batches | lr 4.75 | ms/batch 910.96 | loss  5.65 | ppl   284.55\n",
            "| epoch   2 |  1200/ 2335 batches | lr 4.75 | ms/batch 966.57 | loss  5.72 | ppl   303.73\n",
            "| epoch   2 |  1400/ 2335 batches | lr 4.75 | ms/batch 933.12 | loss  5.69 | ppl   294.84\n",
            "| epoch   2 |  1600/ 2335 batches | lr 4.75 | ms/batch 930.21 | loss  5.67 | ppl   291.48\n",
            "| epoch   2 |  1800/ 2335 batches | lr 4.75 | ms/batch 937.50 | loss  5.64 | ppl   282.15\n",
            "| epoch   2 |  2000/ 2335 batches | lr 4.75 | ms/batch 940.45 | loss  5.64 | ppl   281.63\n",
            "| epoch   2 |  2200/ 2335 batches | lr 4.75 | ms/batch 943.84 | loss  5.59 | ppl   267.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2367.37s | valid loss  6.00 | valid ppl   403.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2335 batches | lr 4.51 | ms/batch 930.47 | loss  5.59 | ppl   266.54\n",
            "| epoch   3 |   400/ 2335 batches | lr 4.51 | ms/batch 940.20 | loss  5.57 | ppl   261.55\n",
            "| epoch   3 |   600/ 2335 batches | lr 4.51 | ms/batch 930.13 | loss  5.49 | ppl   242.49\n",
            "| epoch   3 |   800/ 2335 batches | lr 4.51 | ms/batch 940.72 | loss  5.52 | ppl   249.96\n",
            "| epoch   3 |  1000/ 2335 batches | lr 4.51 | ms/batch 930.85 | loss  5.41 | ppl   224.57\n",
            "| epoch   3 |  1200/ 2335 batches | lr 4.51 | ms/batch 929.08 | loss  5.50 | ppl   245.23\n",
            "| epoch   3 |  1400/ 2335 batches | lr 4.51 | ms/batch 930.32 | loss  5.48 | ppl   240.38\n",
            "| epoch   3 |  1600/ 2335 batches | lr 4.51 | ms/batch 924.55 | loss  5.47 | ppl   236.76\n",
            "| epoch   3 |  1800/ 2335 batches | lr 4.51 | ms/batch 920.29 | loss  5.45 | ppl   232.71\n",
            "| epoch   3 |  2000/ 2335 batches | lr 4.51 | ms/batch 940.87 | loss  5.45 | ppl   232.48\n",
            "| epoch   3 |  2200/ 2335 batches | lr 4.51 | ms/batch 923.76 | loss  5.38 | ppl   216.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2384.72s | valid loss  5.92 | valid ppl   371.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.60 | test ppl   269.23\n",
            "=========================================================================================\n",
            "Accuracy: 18.559701492537314%\n"
          ]
        }
      ]
    }
  ]
}