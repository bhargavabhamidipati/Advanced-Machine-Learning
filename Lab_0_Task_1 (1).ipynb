{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8efvpaXv8t67"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGr-AR6l8t67"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAtkMc878t68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c9037da-7097-41d4-9b6d-8af1b9a57f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "from torch import nn, optim, no_grad, float, device, cuda, sum\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Resize, Compose\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "# import torch_directml - I own an AMD gpu so this is a pytorch port using directml backend\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# device = torch_directml.device(0)\n",
        "\n",
        "device = device('cuda' if cuda.is_available() else 'cpu')\n",
        "\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2_miGvQ8t68"
      },
      "source": [
        "### Loading the datasets\n",
        "\n",
        "In Pytorch, there are datasets and dataloaders. Dataset contains the entire collection of training, testing samples while the dataloader providers an iterable wrapper that gives us the samples in batches.\n",
        "\n",
        "An additional step is performed to split training into training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chsvQvms8t68",
        "outputId": "97150aeb-3e1e-4e05-9c59-5391f4f5348c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:28<00:00, 5963643.72it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "orig_dataset = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_dataset = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11C8Iad08t69"
      },
      "outputs": [],
      "source": [
        "# Some hyperparameters can be optionally left out\n",
        "# For speed and brevity, only focusing on optimizers and activations might be good with lr 1e-4\n",
        "\n",
        "activations = [nn.LeakyReLU, nn.Tanh]\n",
        "optimizers = [optim.SGD, optim.Adam]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XerP9Vs78t69"
      },
      "source": [
        "### Basic CNN\n",
        "- Leaky ReLU activation\n",
        "- SGD optimizer\n",
        "- 1e-3 learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywW8Oz2Q8t69"
      },
      "outputs": [],
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    def __init__(self, activation):\n",
        "        super().__init__()\n",
        "        self.stack = nn.Sequential( # image is 3x32x32\n",
        "            nn.Conv2d(3, 18, kernel_size=3, padding='same'), activation(), # image is 18x32x32\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 18x16x16\n",
        "            nn.Conv2d(18, 54, kernel_size=3, padding='same'), activation(), # image is 54x16x16\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 54x8x8\n",
        "            nn.Conv2d(54, 54, kernel_size=3, padding='same'), activation(), # image is 54x16x16\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # 54x4x4\n",
        "            nn.AvgPool2d(kernel_size=4), # should try global\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(54, 20), activation(),\n",
        "            nn.Linear(20, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbgcsyW38t69"
      },
      "source": [
        "### Training annd Test Procedures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlOXHjT18t69"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, model, loss_fn, optimizer):\n",
        "    size = len(train_dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_hat = model.forward(X)\n",
        "        loss = loss_fn(y_hat, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 1000 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return loss\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "    return test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjgR3zJ08t6-"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B37LdazY8t6-",
        "outputId": "4c3f30b6-eb74-4074-abab-4d136804e50d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.320946  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308742 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.320692  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308616 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.320441  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308492 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.320193  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308370 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.319950  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308251 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.319709  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308134 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.319466  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.308019 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.319229  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307907 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.319001  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307797 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.318778  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307690 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.318559  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307585 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.318343  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307482 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.318130  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307380 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.317921  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307280 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.317710  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307181 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.317501  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.307084 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.317296  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306988 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.317094  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306892 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.316896  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306797 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.316701  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306703 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 2.316513  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306610 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 2.316330  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306516 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 2.316146  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306423 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 2.315962  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306329 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 2.315781  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.306235 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.301511  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.4%, Avg loss: 2.303818 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.301348  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 2.303669 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.301194  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.6%, Avg loss: 2.303527 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.301050  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.6%, Avg loss: 2.303392 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.300914  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.7%, Avg loss: 2.303263 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.300788  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 10.8%, Avg loss: 2.303140 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.300669  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.0%, Avg loss: 2.303022 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 2.300556  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.0%, Avg loss: 2.302908 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.300451  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.2%, Avg loss: 2.302800 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.300351  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.4%, Avg loss: 2.302696 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.300257  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.7%, Avg loss: 2.302595 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.300168  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.9%, Avg loss: 2.302498 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.300085  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.302405 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.300006  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.302315 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.299931  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.302227 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.299861  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.302143 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.299795  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.302061 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.299732  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.301981 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.299672  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.3%, Avg loss: 2.301904 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 2.299616  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 2.301829 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 2.299562  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 12.1%, Avg loss: 2.301755 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 2.299511  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.9%, Avg loss: 2.301684 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 2.299463  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.8%, Avg loss: 2.301614 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 2.299417  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.7%, Avg loss: 2.301545 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 2.299373  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 11.6%, Avg loss: 2.301478 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.306690  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 21.4%, Avg loss: 2.126366 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.158266  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 26.0%, Avg loss: 1.960572 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.977096  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 28.0%, Avg loss: 1.906393 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.920914  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 29.2%, Avg loss: 1.879293 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.900835  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 30.1%, Avg loss: 1.857065 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.880978  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 30.9%, Avg loss: 1.837579 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.861251  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 31.6%, Avg loss: 1.819184 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.842120  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 32.3%, Avg loss: 1.800693 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.820526  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 1.783131 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.800268  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.764940 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.775980  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 34.8%, Avg loss: 1.747149 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.749768  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 35.5%, Avg loss: 1.728948 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.719733  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 36.2%, Avg loss: 1.714097 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.690992  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.699718 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.665339  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 36.9%, Avg loss: 1.686411 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.640955  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 37.3%, Avg loss: 1.674115 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.618604  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 37.7%, Avg loss: 1.662115 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.598615  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 38.2%, Avg loss: 1.651358 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.580776  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 38.6%, Avg loss: 1.641062 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.564459  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 39.0%, Avg loss: 1.631154 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.550267  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 39.3%, Avg loss: 1.622049 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.538689  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 39.6%, Avg loss: 1.612693 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.526927  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 40.2%, Avg loss: 1.603292 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.515071  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 40.6%, Avg loss: 1.593992 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.503993  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 40.9%, Avg loss: 1.585078 \n",
            "\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.312018  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 21.7%, Avg loss: 2.072408 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.086577  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 27.1%, Avg loss: 1.954839 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.920856  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 29.8%, Avg loss: 1.878585 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.813448  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 31.5%, Avg loss: 1.829391 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.734663  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 33.9%, Avg loss: 1.773630 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.663674  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 36.6%, Avg loss: 1.722419 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.604972  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 37.8%, Avg loss: 1.684424 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.562165  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 38.9%, Avg loss: 1.655848 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.529860  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 39.5%, Avg loss: 1.632626 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.504014  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 40.2%, Avg loss: 1.612565 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.481514  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 41.1%, Avg loss: 1.594398 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.460948  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 41.7%, Avg loss: 1.577548 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.441841  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 42.4%, Avg loss: 1.561728 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.424060  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 43.3%, Avg loss: 1.546562 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.407709  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 44.0%, Avg loss: 1.531966 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.392820  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 44.5%, Avg loss: 1.517893 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.379089  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 45.2%, Avg loss: 1.504397 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.366386  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 45.6%, Avg loss: 1.491444 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.354544  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 46.2%, Avg loss: 1.479034 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.343314  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 46.7%, Avg loss: 1.467195 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.332988  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 47.2%, Avg loss: 1.455707 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.323282  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 47.5%, Avg loss: 1.444567 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.313855  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 48.0%, Avg loss: 1.433739 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.304530  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 48.3%, Avg loss: 1.423149 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1.294742  [   64/40000]\n",
            "Test Error: \n",
            " Accuracy: 48.9%, Avg loss: 1.412693 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# I would suggest reducing this to run it faster as full training might not be required\n",
        "epochs = 25\n",
        "lr = 1e-4\n",
        "''' for lr in learning_rates:\n",
        "    for optimizer_fn in optimizers:\n",
        "        for activation in activations:\n",
        "            for batch_size in batch_sizes:\n",
        " '''\n",
        "from itertools import product\n",
        "# Instead of writing out four nested for-loops, each combination of lr, optimizer, activation, and batch is combined in one tuple\n",
        "# Same speed, looks tidier\n",
        "hyperparam_combos = product(optimizers, activations)\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for combo in iter(hyperparam_combos):\n",
        "  # lr = combo[0]\n",
        "  optimizer_fn = combo[0]\n",
        "  activation = combo[1]\n",
        "  # batch_size = combo[3]\n",
        "\n",
        "  train_dataset, val_dataset = random_split(dataset=orig_dataset, lengths=[0.8,0.2])\n",
        "\n",
        "  train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "  val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size)\n",
        "  test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  model = BasicCNN(activation=activation)\n",
        "  model.to(device)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
        "\n",
        "  # Saves one event file per hyperparameter combo as named below\n",
        "  # writer = SummaryWriter(f\"runs/BasicCNN/batch={batch_size} lr={lr} activation={activation()._get_name()} optimizer={optimizer_fn.__name__}\")\n",
        "\n",
        "  # runs current hyperparameter combo training as usual\n",
        "  for t in range(epochs):\n",
        "      print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "      train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
        "      # writer.add_scalar(tag='Training Loss', scalar_value=train_loss, global_step=t)\n",
        "      val_loss = test(val_dataloader, model, loss_fn)\n",
        "      # writer.add_scalar(tag='Validation Loss', scalar_value=val_loss, global_step=t)\n",
        "\n",
        "  correct = 0\n",
        "  for X, y in test_dataloader:\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat = model.forward(X)\n",
        "    correct += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "  model_results[f\"activation={activation()._get_name()} optimizer={optimizer_fn.__name__}\"] = correct*100/len(test_dataloader.dataset)\n",
        "  # writer.close()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in model_results.items():\n",
        "  print(f\"Test accuracy of {key} model was {value}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AyOj7o_BmpM",
        "outputId": "271d0859-18d4-4f2f-814e-94e12439fc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy of activation=LeakyReLU optimizer=SGD model was 10.0%\n",
            "Test accuracy of activation=Tanh optimizer=SGD model was 11.2%\n",
            "Test accuracy of activation=LeakyReLU optimizer=Adam model was 41.52%\n",
            "Test accuracy of activation=Tanh optimizer=Adam model was 48.85%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}